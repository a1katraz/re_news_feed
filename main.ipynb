{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec82b03-051f-4742-8918-4eb0c50f5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.times_reader as feedreader\n",
    "import ipynb.fs.full.openai_models as openai\n",
    "import ipynb.fs.full.gemini_models as gemini\n",
    "import ipynb.fs.full.data_operations as data\n",
    "import ipynb.fs.full.database_operations as database\n",
    "import pandas\n",
    "import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0988ca48-1396-40a9-82f2-c249aac053dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_gsheets():\n",
    "    cities = ['Mumbai', 'Delhi', 'Bangalore']\n",
    "    for city in cities:\n",
    "        try:\n",
    "            feedreader.toi_feed_reader.load_links()\n",
    "            \n",
    "            print(f'Trying feed for city: {city}...')\n",
    "            feed = feedreader.toi_feed_reader(city)\n",
    "            entries = feed.get_feed_entries()\n",
    "            print(f'Found {entries.shape[0]} news articles for {city}. Now processing to analyse...')\n",
    "            \n",
    "            llm = gemini.Gemini_Models()\n",
    "            response = llm.classify_headlines(entries[['sub-site', 'title']], silent_mode=False)\n",
    "            if(response.strip()=='' or response.strip()=='{}' or response.strip()=='[]'):\n",
    "                continue\n",
    "            else:\n",
    "                classified_hl = pandas.DataFrame(json.loads(response), columns=['title', 'classification', 'explanation'])\n",
    "                \n",
    "                itemized_hl = entries.join(classified_hl, lsuffix='_orig', rsuffix='_copy') \n",
    "                itemized_hl = itemized_hl[itemized_hl['classification'] == True]\n",
    "                itemized_hl.drop_duplicates(subset=['link_id'], keep='first', inplace=True, ignore_index=False)\n",
    "                \n",
    "                writer = data.gspread_Operator()\n",
    "                writer.write_headlines(itemized_hl)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} in code flow...')\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1983cfc5-f035-4ac4-983e-b5267193e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_with_databases():\n",
    "\n",
    "    # Writing all of this with databases instead of google_sheets because:\n",
    "    # 1. Processing with local databases is faster\n",
    "    # 2. Storing data is much easier with a local database because of limitations in write ops in Google Sheets \n",
    "    \n",
    "    sub_sites = ['Hyderabad']\n",
    "    for sub_site in sub_sites:\n",
    "        try:\n",
    "            feedreader.toi_feed_reader.load_links()\n",
    "            \n",
    "            print(f'Trying feed for sub_site: {sub_site}...')\n",
    "            feed = feedreader.toi_feed_reader(sub_site)\n",
    "            entries = feed.get_feed_entries()\n",
    "            print(f'Found {entries.shape[0]} news articles for {sub_site}. Now processing to dedupe...')\n",
    "\n",
    "            cnxn = database.mysql_Database('timely_feeds')\n",
    "            existing_links = cnxn.search_values_in_table('fact_feed_table', 'link_id', entries['link_id'], ['link_id'])\n",
    "\n",
    "            uniques = entries[~entries['link_id'].isin(existing_links['link_id'])]\n",
    "            if(uniques.shape[0]>0):                                                          #There is atleast one article worth testing \n",
    "                llm = gemini.Gemini_Models()\n",
    "                response = llm.classify_headlines(uniques[['link_id', 'sub_site_name', 'title']], silent_mode=False)\n",
    "                if(response.strip()=='' or response.strip()=='{}' or response.strip()=='[]'):\n",
    "                    continue\n",
    "                else:\n",
    "                    classified_hl = pandas.DataFrame(json.loads(response), columns=['link_id', 'title', 'classification', 'explanation'])\n",
    "                    itemized_hl = pandas.merge(uniques, classified_hl, on='link_id', how='inner', suffixes=('', '_copy')) ##= entries.join(classified_hl, lsuffix='_orig', rsuffix='_copy') \n",
    "                    itemized_hl = itemized_hl[itemized_hl['classification'] == True]\n",
    "                    itemized_hl.drop_duplicates(subset=['link_id'], keep='first', inplace=True, ignore_index=False)\n",
    "                    #print(itemized_hl.columns)\n",
    "                    #Write these articles in the database \n",
    "                    cnxn.insert_values_in_table('fact_classified_articles', \n",
    "                                                itemized_hl[['site_name', 'sub_site_name', 'link_id', 'links', 'title', 'link_date', 'classification', 'explanation']]\n",
    "                                               )\n",
    "                    \n",
    "                    #Write the id of all feed gathered articles to the feed_table database so as to not read them again\n",
    "                    cnxn.insert_values_in_table('fact_feed_table', \n",
    "                                                   uniques[['link_id', 'site_name', 'sub_site_name', 'link_date']])\n",
    "                    \n",
    "            else:\n",
    "                print(f'No new articles found for this subsite: {sub_site}')\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} in code flow...')\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        finally:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96eb9d2b-8691-4fd4-b92b-d380f9187ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying feed for sub_site: Hyderabad...\n",
      "Found 20 news articles for Hyderabad. Now processing to dedupe...\n",
      "No new articles found for this subsite: Hyderabad\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    hl = main_with_databases()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a65ee-efa9-4a31-a4d5-ab295d0cd912",
   "metadata": {},
   "source": [
    "cnxn = database.mysql_Database('timely_feeds')\n",
    "item = pandas.DataFrame({\n",
    "            'site_name': ['TOI', 'TOI'],\n",
    "            'sub_site_name': ['Delhi', 'Mumbai'],\n",
    "            'link_id': ['120831219', '120834074'],\n",
    "            'link': ['https://timesofindia.indiatimes.com/city/delhi/dda-razes-illegal-farmhouses-on-encroached-land-in-sainik-farms/articleshow/120831219.cms',\n",
    "                     'https://timesofindia.indiatimes.com/city/mumbai/hc-declares-citys-biggest-dumping-ground-a-protected-forest/articleshow/120834074.cms'],\n",
    "            'title': ['DDA razes illegal farmhouses on encroached land in Sainik Farms', \n",
    "                      'HC declares city\\'s biggest dumping ground a \\'protected forest\\''],\n",
    "            'link_date': ['2025-05-02T22:58:48+05:30', '2025-05-03T01:04:28+05:30'],\n",
    "            'classification': ['TRUE', 'TRUE'],\n",
    "            'explanation': ['Headline reports DDA action against illegal structures on encroached land.',\n",
    "                            'Court order changes land status, affecting potential development.'],\n",
    "            }\n",
    "        )\n",
    "cnxn.insert_values_in_table('fact_classified_articles', \n",
    "                                                item[['site_name', 'sub_site_name', 'link_id', 'link', 'title', 'link_date', 'classification', 'explanation']]\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a68d2fa-df9f-48c6-a039-53deda35e51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     120854598\n",
       "1     120854658\n",
       "2     120854596\n",
       "3     120854561\n",
       "4     120854559\n",
       "5     120850644\n",
       "6     120850552\n",
       "7     120847436\n",
       "8     120847012\n",
       "9     120846984\n",
       "10    120846861\n",
       "11    120833519\n",
       "12    120833478\n",
       "13    120833039\n",
       "14    120833033\n",
       "15    120833005\n",
       "16    120832998\n",
       "17    120832990\n",
       "18    120832985\n",
       "19    120832983\n",
       "Name: link_id, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feedreader.toi_feed_reader.load_links()\n",
    "feed = feedreader.toi_feed_reader('Hyderabad')\n",
    "entries = feed.get_feed_entries()\n",
    "entries['link_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48062e4-2849-4fb3-bdc0-73025128f1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_feed",
   "language": "python",
   "name": "news_feed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
